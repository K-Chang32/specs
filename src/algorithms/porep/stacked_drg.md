---
title: Stacked DRG PoRep
---

This section describes *Stacked DRG PoRep* (SDR), the specific Proof-of-Replication (PoRep) used in Filecoin. In this construction, the prover encodes the original data into a replica and commits to it. An offline PoRep proves that the commitment to the replica is a valid commitment of the encoded original data.

SDR has been presented by [Ben Fisch at EUROCRYPT19](https://eprint.iacr.org/2018/702.pdf).

# Introduction

## Background on Proof-of-Replication
*Proof-of-Replication* enables a prover *P* to convince a verifier *V* that *P* is storing a replica *R*, a physically independent copy of some data *D*, unique to *P*. The scheme is defined by a tuple of polynomial time algorithms (_Setup_, Replication, _Prove_, _Verify_). The assumption is that generation of a replica after _Replicate_  must be difficult (if not impossible) to generate.

- *Setup*: On setup, the public parameters of the proving systems are set.
- *Replicate*: On replication, either a party or both (depending on the scheme, in our case the prover only!) generate a unique permutation of the original data _D_, which we call replica _R_.
- _Prove_: On receiving a challenge, the prover must generate a proof that it is in possession of the replica and that it was derived from data _D_. The prover must only be able to respond to the challenge successfully if it is in possession of the replica, since would be difficult (if not impossible) to generate a replica that can be used to generate the proof at this stage
- _Verify_: On receiving the proof, the verifier checks the validity of the proof and accepts or rejects the proof.

{{% mermaid %}}
sequenceDiagram
    Note right of Prover: CommD
    Prover-->>Prover: R, CommR ← Replicate(D)
    Prover->>Verifier: CommR
    Verifier-->>Verifier: Generate random challenge
    Verifier->>Prover: challenge
    Prover-->>Prover: proof ← Prove(D, R, challenge)
    Prover->>Verifier: proof
{{% /mermaid %}}

## Time-bounded Proof-of-Replication

**Timing assumption**. *Time-bounded Proof-of-Replication* are constructions of PoRep with timing assumptions. The assumption is that generation of the replica (hence the _Replication_) takes some time _t_ that is substantially larger than the time it takes to produce a proof (hence _time(Prove)_) and the round-trip time (_RTT_) for sending a challenge and receiving a proof.

**Distinguishing Malicious provers**. A malicious prover that does not have _R_, must obtain it (or generate it), before the _Prove_ step. A verifier can distinguish an honest prover from a malicious prover, since the malicious one will take too long to answer the challenge. A verifier will reject if receiving the proof from the prover takes longer than a timeout (bounded between proving time and replication time).

## Background on Stacked DRG PoRep

*Stacked DRG PoRep* (SDR) is a specific Proof-of-Replication construction that we use in Filecoin. ZigZag has been designed by [Ben Fisch at EUROCRYPT19](https://eprint.iacr.org/2018/702.pdf).  At a high level, SDR ensures that the *Replicate* step is a slow non-parallelizable sequential process by using a special type of graph called Depth Robust Graphs (DRG).


**Encoding using DRGs**. A key is generated by sequentially labeling nodes in the graph such that each label depends on the labels of its parents. The depth robustness property of these graphs ensure that the sequential labeling steps are not parallelizable. The final labels are used as a key to encode the original data.

TODO: This probably needs a more thorough rewrite.

** Stacked DRGs**. SDR builds on the above by stacking DRG graphs into `LAYERS` layers. Each layer is connected to the previous by a Bipartite Expander Graph. The combination of DRGs and expander graphs guarantee the security property of PoRep. As before, the key produced by the final layer is used to encode the original data, yielding the replica.

**Generating SDR proofs**. Given the following public parameters:

- `CommD` is the Merkle Tree root hash of the input data to the first layer
- `ReplicaId` is a unique replica identifier (see the Filecoin Proofs spec for details). `ReplicaId` must depend on `CommD`.
- `CommR` is the on-chain commitment to the replica.

An SDR proof proves that some data whose committment is `CommD` has been used to run a `Replicate` algorithm and generated some data. `CommR` is the on-chain commitment to both the replicated data and to intermediate stages required to prove `Replicate` was performed correctly.

An SDR proof consists of a set of challenged DRG nodes for each layer, a set of parent nodes for each challenged node and a Merkle tree inclusion proof for each node provided. The verifier can then verify the correct labeling of each node and that the nodes given were consistent with the prover's commitments.

**Making proofs succinct with SNARKs**: The proof size in SDR is too large for blockchain usage (~100MB TODO: check this), mostly due to the large amount of Merkle tree inclusion proofs required to achieve security. We use SNARKs to generate a proof of knowledge of a correct SDR proof. In other words, we implement the SDR proof verification algorithm in an arithmetic circuit and use SNARKs to prove that it was evaluated correctly.

The SNARK circuit proves that given Merkle roots `CommD`, and `CommR`, the prover correctly derived the labels at each layer and correctly performed the final encoding.

## PoRep in Filecoin

Proof-of-Replication proves that a Storage Miner is dedicating unique storage for each ***sector***. Filecoin Storage Miners collect new clients' data in a sector, run a slow encoding process (called `Seal`) and generate a proof (`SealProof`) that the encoding was generated correctly.

In Filecoin, PoRep provides two guarantees: (1) *space-hardness*: Storage Miners cannot lie about the amount of space they are dedicating to Filecoin in order to gain more power in the consensus; (2) *replication*: Storage Miners are dedicating unique storage for each copy of their clients data.

Glossary:

- __*sector:*__ a fixed-size block of data of `SECTOR_SIZE` bytes which generally contains clients' data.
- __*unsealed sector:*__ a concrete representation (on disk or in memory) of a sector's that follows the "Storage Format" described in [Client Data Processing](client-data.md#storage-format) (currently `paddedfr32v1` is the required default).
- __*sealed sector:*__  a concrete representation (on disk or in memory) of the unique replica generated by `Seal` from an __*unsealed sector*__. A sector contains one or more ***pieces***.
- __*piece:*__ a block of data of at most `SECTOR_SIZE` bytes which is generally a client's file or part of.

# Stacked DRG Construction

## Public Parameters

The following public parameters are used in the ZigZag Replication and Proof Generation algorithms:

TODO: the Appendix should explain why we picked those values
TODO: Just interpolate a table of the Orient parameters and reconcile naming.

| name | type | description | value |
| --- | --- | --- | ---: |
| `SECTOR_SIZE` | `uint` | Number of nodes in the DRG in bytes | `68,719,476,736` |
| `LAYERS` | `uint` | Number of Depth Robust Graph stacked layers. | `10` |
| `BASE_DEGREE` | `uint` | In-Degree of each Depth Robust Graph. | `6` |
| `EXPANSION_DEGREE` | `uint` | Degree of each Bipartite Expander Graph to extend dependencies between layers. | `8` |
| `GRAPH_SEED` |  `uint` | Seed used for random number generation in `baseParents`. | `TODO` |
| `NODE_SIZE` | `uint` | Size of each node in bytes.| `32B` |


The following constants are computed from the public parameters:


| name | type | description | computation | value |
| --- | --- | --- | ---: | ---- |
| `PARENTS_COUNT` | `uint` | Total number of parent nodes |`EXPANSION_DEGREE + BASE_DEGREE` | `13` |
| `GRAPH_SIZE` | `uint` | Number of nodes in the graph  | `SECTOR_SIZE / NODE_SIZE` | `2,147,483,648` |
| `TREE_DEPTH` | `uint` | Height of the Merkle Tree of a sector | `LOG_2(GRAPH_SIZE)` | `31` |


The following additional public parameters are required:

- `TAPER` : `Float`: Fraction of each layer's challenges by which to reduce next-lowest layer's challenge count.
- `TAPER_LAYERS`: `uint`: Number of layers
  `Data` is a byte array initialized to the content of __*unsealed sector*__ and will be mutated in-place by the replication process.

## Hash Functions

We have describe three hash functions:

| name          | description                                                                                                                                                                | size of input              | size of output | construction          |
| ------------- | ------------------------------------------------------------                                                                                                               | -------------              | -------------- | --------------------- |
| `KDFHash`     | Hash function used as a KDF to derive the key used to label a single node.                                                                                                 | TODO                       | `32B`          | `Blake2s-256`         |
| `ColumnHash`  | Hash function used to hash the labeled leaves of each layer (see SDR Column Commitments).                                                                                  | TODO                       | `32B`          | `JubjubPedersen`      |
| `RepCompress` | Collision Resistant Hash function used for the Merkle tree.                                                                                                                | 2 x `32B` + integer height | `32B`          | `JubjubPedersen`      |
| `RepHash`     | Balanced binary Merkle tree based used to generate commitments to sealed sectors, unsealed sectors, piece commitments, and intermediate parts of the Proof-of-Replication. | TODO                       | `32B`          | Uses `RepCompress`    |
|               |                                                                                                                                                                            |                            |                |                       |

### RepHash

`RepHash` is a vector commitment used to generate commitments to sealed sectors, unsealed sectors, piece commitments and intermediate stepds of the Proof-of-Replication. Filecoin uses a balanced binary Merkle tree for `RepHash`. The leaves of the Merkle tree are pairs of adjacent nodes.

`RepHash` inputs MUST respect a valid Storage Format. [TODO: What does this mean?]

```go
type node [32]uint8

// Create and return a balanced binary Merkle tree and its root commitment.
// len(leaves) must be a power of 2.
func RepHash(leaves []node) ([][]node, node) {
	rows = [][]node

	currentRow := leaves
	for height := 0; len(currentRow) > 1; height += 1 {
		rows.push(currentRow)
		var nextRow []node

		for i := 0; i < len(row)/2; i += 2 {
			left := row[i]
			right := row[i+1]

			// NOTE: Depending on choice of RepCompress, heightPart may be trimmed to fewer than 8 bits.
			heightPart := []uint8{height}

			input1 := append(heightPart, left...)
			input := append(input1, right...)
			hashed = RepCompress(input, height)
			nextRow = append(nextRow, hashed)
		}

		currentRow = nextRow
	}
    // The tree returned here is just a vector of rows for later use. Its representation is not part of the spec.
	return rows, currentRow[0]
}

```

## Stacked DRG Graph

The slow sequential encoding required is enforced by the depth robusness property of the SDR graph.

**Encoding with SDR**: The data from a sector (of size `SECTOR_SIZE`) is divided in `NODE_SIZE` nodes (for a total of `GRAPH_SIZE` nodes) and arranged in a directed acyclic graph. The structure of the graph is used to label the nodes sequentially to generate a key with which to encode the original data: in order to label a node, its parents must be labeled (see the "Layer Labeling" section below). We repeat this process for `LAYERS` layers, where the input to a next layer is the output of the previous one.

**Generating the SDR graph**: The SDR graph is divided in `LAYERS` layers. Each layer is a directed acyclic graph and it combines a Depth Robust Graph (DRG) and a Bipartite Expander graph. [TODO: this isn't quite right.]

We provide an algorithm (`SDR`) which computes the parents of a node. In high level, the parents of a node are computed by combining two algorithms: some parents (`BASE_DEGREE` of them) are computed via the `BucketSample` algorithm extended with a direct ordering of nodes, others (`EXPANSION_DEGREE` of them) are computed via the `Chung` algorithm. 

### `SDRGraph`: SDR Graph algorithm

Overview: Compute the DRG and Bipartite Expander parents using respectively `BucketSample` and `ChungExpander`.

#### Inputs

| name    | description                                       | Type   |
| ------- | ------------------------------------------------- | ------ |
| `node`  | The node for which the parents are being computed | `uint` |
| `layer` | The layer of the ZigZag graph                     | `uint` |

#### Outputs

| name      | description                                 | Type                  |
| --------- | ------------------------------------------- | --------------------- |
| `parents` | The ordered parents of node `node` on layer `layer` | `[PARENTS_COUNT]uint` |

#### Algorithm

- If `layer = 1`:
  - Compute `drgParents = BucketSample(node)`
  - Set `parents` to be `drgParents`.

- If `layer > 1`:
  - Compute `drgParents = BucketSample(node)`
  - Compute `expanderParents = ChungExpander(node)`
  - Set `parents` to be the concatenation of `drgParents` and `expanderParents`

#### Pseudocode

We provide below a more succinct representation of the algorithm:

```
func ZigZag(node uint, layer uint) {

  if layer == 1 {
  	// On first layer
  	let drgParents = BucketSample(node)
  	return drgParents
  } else {
    // On subsequent layers
    let drgParents = BucketSample(node)
  	let expanderParents = ChungExpander(node)
  	return concat(drgParents, expanderParents)
  }
}
```

#### Tests

FIXME: Change this.

- Each `parent` in `parents` MUST not be greater than `GRAPH_SIZE-1` and lower than `0`
- If `layer` is even:
  - Each `parent` in `parents` MUST be greater than `node`
    - EXCEPT: if `node` is `0`, then all parents MUST be `0`
- if `layer` is odd:
  - Each `parent` in `parents` MUST be less than `node`
    - EXCEPT: if `node` is `GRAPH_SIZE-1`, then all parents MUST be `GRAPH_SIZE-1`

#### Time-space tradeoff

Computing the parents using both `BucketSample` and `ChungExpander` for every layer can be an expensive operation, however, this can be avoided by caching the parents.

### `BucketSample+`: Depth Robust Graphs algorithm

This section describes how to compute the "base parents" of the ZigZag graph, which is the equivalent of computing the parents of a Depth Robust Graph.

The properties of DRG graphs guarantee that a sector has been encoded with a slow, non-parallelizable process. We use the `BucketSample` algorithm that is based on DRSample ([ABH17](https://acmccs.github.io/papers/p1001-alwenA.pdf)) and described in [FBGB18](https://web.stanford.edu/~bfisch/porep_short.pdf) and generates a directed acyclic graph of in-degree `BASE_DEGREE`.

`BucketSample` DRG graphs are random graphs that can be deterministically generated from a seed; different seed lead with high probability to different graphs. In SDR, we use the same seed `GRAPH_SEED` for each layer of the SDR graph such that they are all based on the same underlying DRG graph.

The parents of any node can be locally computed without computing the entire graph. We call the parents of a node calculated in this way *base parents*.

```go
func BucketSampleInner(node uint) (parents [BASE_DEGREE]uint) {
    switch node {
        // Special case for the first node, it self references.
        // Special case for the second node, it references only the first one.
        case 0:
        case 1:
            for i := 0; i < BASE_DEGREE; i++ {
                parents[i] = 0
            }
        default:
            rng := ChaChaRng.from_seed(GRAPH_SEED)

            for k := 0; k < BASE_DEGREE; k++ {
                // iterate over m meta nodes of the ith real node
                // simulate the edges that we would add from previous graph nodes
                // if any edge is added from a meta node of jth real node then add edge (j,i)
                logi := floor(log2(node * BASE_DEGREE))
                j := rng.gen() % logi
                jj := min(node * BASE_DEGREE + k, 1 << (j + 1))
                backDist := rng.gen_range(max(jj >> 1, 2), jj + 1)
                out := (node * BASE_DEGREE + k - backDist) / BASE_DEGREE

                // remove self references and replace with reference to previous node
                if out == node {
                    parents[i] = node - 1
                } else {
                    parents[i] = out;
                }
            }

            sort(parents)
    }
}
```

`BucketSample` extends `BucketSampleInner` to include the node's 'immediate predecessor'. Each node except the first in a
DRG generated by `BucketSample` has the node whose index is one less than its own as a parent. This ensures that
visiting nodes whose indexes are sequential will result in a graph traversal in topological order.

### `ChungExpander`: Bipartite Expander Graphs

TODO: explain why we link nodes in the current layer

Each node in layers other than the first has `EXPANSION_DEGREE` parents generated via the `ChungExpander`
algorithm. Note that the indexes returned refer to labels from the *previous* layer. TODO: Make this all clearer with explicit notation.

```go
func ChungExpander(node uint) (parents []uint) {
	parents := make([]uint, EXPANSION_DEGREE)

	feistelKeys := []uint{1, 2, 3, 4} // TODO

	for i := 0, p := 0; i < EXPANSION_DEGREE; i++ {
		a := node * EXPANSION_DEGREE + i
    transformed := feistelPermute(GRAPH_SIZE * EXPANSION_DEGREE, a, feistelKeys)
    other := transformed / EXPANSION_DEGREE
    if other < node {
      parents[p] = other
      p += 1
    }
  }
}

```

#### Time-Space tradeoff

Computing these parents can be expensive (especially due to the hashing required by the Feistel algorithm). A miner can trade this computation by storing the expansion parents.

#### Feistel construction

We use three rounds of `Feistel` to generate a permutation to compute the parents of the Bipartite Expander graph.

TODO: Add `FEISTEL_ROUNDS` and `FEISTEL_BYTES` (or find its definitions)

```go
func permute(numElements uint, index uint, keys [FEISTEL_ROUNDS]uint) uint {
    u := feistelEncode(index, keys)

    while u >= numElements {
        u = feistelEncode(u, keys)
    }
    // Since we are representing `numElements` using an even number of bits,
    // that can encode many values above it, so keep repeating the operation
    // until we land in the permitted range.

    return u
}

func feistelEncode(index uint, keys [FEISTEL_ROUNDS]uint) uint {
    left, right, rightMask, halfBits := commonSetup(index)

    for _, key := range keys {
        left, right = right, left ^ feistel(right, key, rightMask)
    }

    return  (left << halfBits) | right
}

func commonSetup(index uint) (uint, uint, uint, uint) {
    numElements := GRAPH_SIZE * EXPANSION_DEGREE
    nextPow4 := 4;
    halfBits := 1
    while nextPow4 < numElements {
        nextPow4 *= 4
        halfBits += 1
    }

    rightMask = (1 << halfBits) - 1
    leftMask = rightMask << halfBits

    right := index & rightMask
    left := (index & leftMask) >> halfBits

    return  (left, right, rightMask, halfBits)
}

// Round function of the Feistel network: `F(Ri, Ki)`. Joins the `right`
// piece and the `key`, hashes it and returns the lower `uint32` part of
// the hash filtered trough the `rightMask`.
func feistel(right uint, key uint, rightMask uint) uint {
    var data [FEISTEL_BYTES]uint

    var r uint
    if FEISTEL_BYTES <= 8 {
        data[0] = uint8(right >> 24)
        data[1] = uint8(right >> 16)
        data[2] = uint8(right >> 8)
        data[3] = uint8(right)

        data[4] = uint8(key >> 24)
        data[5] = uint8(key >> 16)
        data[6] = uint8(key >> 8)
        data[7] = uint8(key)

        hash := blake2b(data)

        r =   hash[0]) << 24
            | hash[1]) << 16
            | hash[2]) << 8
            | hash[3])
    } else {
        data[0]  = uint8(right >> 56)
        data[1]  = uint8(right >> 48)
        data[2]  = uint8(right >> 40)
        data[3]  = uint8(right >> 32)
        data[4]  = uint8(right >> 24)
        data[5]  = uint8(right >> 16)
        data[6]  = uint8(right >> 8)
        data[7]  = uint8(right)

        data[8]  = uint8(key >> 56)
        data[9]  = uint8(key >> 48)
        data[10] = uint8(key >> 40)
        data[11] = uint8(key >> 32)
        data[12] = uint8(key >> 24)
        data[13] = uint8(key >> 16)
        data[14] = uint8(key >> 8)
        data[15] = uint8(key)

        hash := blake2b(data)

        r =   hash[0] << 56
            | hash[1] << 48
            | hash[2] << 40
            | hash[3] << 32
            | hash[4] << 24
            | hash[5] << 16
            | hash[6] << 8
            | hash[7]
    }

    return r & rightMask
}
```

# Replication

> The Replication phase turns an *unsealed sector* into a *sealed sector* by first *generating a key*, then using the key to *encode the orignal data*.

Before running the `Replicate` algorithm, the prover must ensure that the sector is correctly formatted with a valid "Storage Format" described in [Filecoin Client Data Processing](client-data.md#storage-format) (currently `paddedfr32v1` is the required default).

TODO: inputs are missing

The Replication Algorithm  proceeds as follows:

- Calculate `ReplicaID` using `Hash` (Blake2s):

`ReplicaID` is a 32-byte array constructed by hashing the concatenation of the following values
- `ProverId` is a 32-byte array uniquely identifying a prover.
- `SectorNumber` is an unsigned 64-bit integer in little-endian encoding represented as an 8-byte array.
- `RandomSeed` is a 32-byte array of randomness extracted from the chain.
- `CommD` is the Merkle root obtained by performing `RepHash` on the original data represented in `paddedfr32v1`.

```
ReplicaID := Hash(ProverID || SectorNumber || RandomSeed || CommD)
```

- Perform `RepHash` on `Data` to yield `CommD` and `TreeD`:
```
CommD, TreeD = RepHash(data)
```

For each of `LAYERS` layers, `l`, perform one __*Layer Replication*__, yielding a replica, tree, and commitment (`CommR_<l>`) per layer:

```go
let layer_replicas = [LAYERS][nodes]uint8
let layer_trees = [LAYERS]MerkleTree
let CommRs = []commitment

let layer = data
for l in 0..layers {
	let layer_replica = ReplicateLayer(layer)
	layer_replicas[l] = layer_replica
	CommRs[l], layers_trees[l] = RepTree(layer_replica)
	layer = layer_replica
}
```

The replicated data is the output of the final __*Layer Replication*__,`layer_replicas[layers-1]`.
Set `CommRLast` to be  `CommR_<Layers>`.
Set `CommRStar` to be `CommRHash(ReplicaID || CommR_0 || CommR_<i> || ... || CommRLast)`.

```go
Replica := layer_replicas[layers - 1]
CommRLast :- CommRs[layers-1]
CommRStar := CommRHash(replicaID, ...CommRs)
```

## Layer Labeling

TODO: Define `Graph`. We need to decide if this is an object we'll explicitly define or if its properties (e.g., `GRAPH_SIZE`) are just part of the replication parameters and all the functions just refer to the _same_ graphs being manipulated across the entire replication process. (At the moment I've avoided defining a `Graph` structure as in other specs I didn't see any object methods, just standalone functions.)

```go

FIXME, make these — possibly by implementing as Go in

func generateKey (replicaId Domain, data []Domain) []Domain
func encodeData (data []Domain, key []Domain) []Domain
func replicate (replicaId Domain, data []Domain) ([]Domain, []Tree)

fun seal (replicaId Domain, data []Domain) ([]Domain,  ????)



TODO: recycle some of this in implementaiton of the above.

func transformAndReplicateLayers(slothIter uint, replicaId Domain,
    data []byte) ([]Domain, []Tree) {

    var taus [LAYERS]Domain
    var auxs [LAYERS]Tree
    var sortedTrees [LAYERS]Tree

    for layer := 0; layer <= LAYERS; layer++ {
        treeData := merkleTree(data)
        sortedTrees.append(treeData)
        if layer < LAYERS {
            vdeEncode(slothIter, replicaId, data)
        }
        zigzag()
    }

    previousCommr := nil
    for _, replicaTree := range sortedTrees {
        commR := replicaTree.root()
        if previousCommr != nil {
            commD := previousCommr
            tau := (commR, commD)
            taus.append(tau)
        }
        auxs.append(replicaTree);
        previousCommr = commR
    }

    return (taus, auxs)
}
```


```go

func label(replicaId uint, data []byte) {
    var parents [PARENT_COUNT]uint
    for n := 0; n < GRAPH_SIZE; n++ {
        var node uint
        if !graphIsReversed() {
            node = n
        } else {
            // If the graph is reversed, traverse in reverse order.
            node = GRAPH_SIZE - n - 1
        }

        parents = parents(node)

        key := KDFHash(replicaId, node, parents, data)

        start := node * NODE_SIZE
        end := start + NODE_SIZE;
        nodeData := data[start:end])
        encoded := slothEncode(key, nodeData, slothIter)

        data[start:end] = encoded
    }
}
```



# Proof Generation

Overview:

- Challenge Derivation
- Proof Generation
- Circuit Proof Generation

TODO: write a single algorithm which includes the spec below

## Challenge Derivation

This is the Fiat-Shamir transform that turns the interactive Proof-of-Replication into non-interactive in the Random Oracle model.

TODO: define `Domain` (for practical purposes a `uint`) and `LayerChallenges` (or find existing definition).

```go
// TODO: we should replace the word commitment with the word seed, this will be more interactive porep friendly
func DeriveChallenges(challenges LayerChallenges, layer uint, leaves uint,
    replicaId Domain, commitment Domain, k uint) []uint {

    n := challenges.ChallengesForLayer(layer)
    var derivedChallenges [n]uint
    for i := 0; i < n; i++ {
        bytes := []byte(replicaId)
        bytes.append([]byte(commitment));
        bytes.append(layer);
        bytes.append(toLittleEndian(n * k + i))

        // For now, we cannot try to prove the first or last node, so make
        // sure the challenge can never be 0 or leaves - 1.
        big_mod_challenge := blake2s(bytes) % (leaves - 2);
        derivedChallenges[i] = big_mod_challenge + 1
    }
}
```

## Challenge Generation

TODO: we may need to remove this section.

Calculate `LAYER_CHALLENGES : [LAYERS]uint`: Number of challenges per layer. (This will be passed to the ZigZag circuit proof.)

Derive challenges for each layer (call `DeriveChallenges()`).

## Witness Generation

```go
let layer_proofs = []

for l in 0..LAYERS {
  let replica = layer_replicas[l]
  let replica_tree = layer_trees[l]

  for c in derive_challenges(LAYER_CHALLENGES[l])
    data_inclusion_proof = inclusion_proof(data[c], DataTree, CommR_<l>)
    replica_inclusion_proof = inclusion_proof(replica[c], replica_tree, CommR_<l+1>) || FAIL// Prove the replica. TODO explain replica[].

    // *** let kdf_preimage = [replica_id] ***
    let parent_replica_inclusion_proofs = []
    for p in parents(c) {
      // *** kdf_preimage.push(p)***
      parent_replica_inclusion_proofs.push(inclusion_proof(p, CommR_<l+1>))
    }
    // *** let key = kdf(kdf_preimage); ***

    // *** encode(key, data[c]) == replica[c]
    // *** We don't actually need to encode in the proof. ***
    // TODO: move this ***stuff*** to verification.

    layer_proof.push((data_inclusion_proof, replication_inclusion_proof, parent_replica_inclusion_proofs))
  }
}

return layer_proofs, CommRstar, CommRLast
```

TODO: reconcile outputs of non-circuit proof with inputs to circuit proof.



## ZigZag: Offline PoRep Circuit Spec

### ZigZag Overview

ZigZag PoRep is based on layering DRG graphs `LAYERS` times. The data represented in each DRG layer is the data encoded in the previous layer. The final layer is the replica (which in Filecoin terms is the sealed sector).

- `ReplicaId` is a unique replica identifier (see the Filecoin Proofs spec for details)
- `CommD` is the Merkle Tree root hash of the input data to the first layer
- `CommR[l]` is the Merkle Tree hash of the output of the DRG encoding at each layer `l`
- `CommRStar` is the hash of the concatenation of the `ReplicaId` and all the `CommR`s.

The (offline) proof size in the ZigZag is too large for blockchain usage (~3MB). We use SNARKs to generate a proof of knowledge of a correct ZigZag proof. In other words, we implement the ZigZag proof verification algorithm in an arithmetic circuit and use SNARKs to prove that it was evaluated correctly.

This circuit proves that given a Merkle root `CommD`, `CommRLast`, and `commRStar`, that the prover knew the correct replicated data at each layer.

### Spec notation

- **Fr**: Field element of BLS12-381
- **UInt**: Unsigned integer
- **{0..x}**: From `0` (included) to `x` (not included) (e.g. `[0,x)` )
- **Check**:
  - If there is an equality, create a constraint
  - otherwise, execute the function
- **Inclusion path**: Binary representation of the Merkle tree path that must be proven packed into a single `Fr` element.

# Offline PoRep circuit

## Public Parameters

*Parameters that are embeded in the circuits or used to generate the circuit*

- `LAYERS : UInt`: Number of DRG layers.
- `LAYER_CHALLENGES : [LAYERS]UInt`: Number of challenges per layer.
- `EXPANSION_DEGREE: UInt`: Degree of each bipartite expander graph to extend dependencies between layers.
- `BASE_DEGREE: UInt`: Degree of each Depth Robust Graph.
- `TREE_DEPTH: UInt`: Depth of the Merkle tree. Note, this is (log_2(Size of original data in bytes/32 bytes per leaf)).
- `PARENT_COUNT : UInt`: Defined as `EXPANSION_DEGREE+BASE_DEGREE`.

## Public Inputs

*Inputs that the prover uses to generate a SNARK proof and that the verifier uses to verify it*

- `ReplicaId : Fr`: A unique identifier for the replica.
- `CommD : Fr`: the Merkle tree root hash of the original data (input to the first layer).
- `CommRLast : Fr`: The Merkle tree root hash of the final replica (output of the last layer).
- `CommRStar : Fr`: A commitment to each `l` layer's Merkle tree root hash `CommR[l]` and `ReplicaId`.
- `InclusionPath : [LAYERS][]Fr`: Inclusion path for the challenged data and replica leaf.
- `ParentInclusionPath : [LAYERS][][PARENT_COUNT]Fr`:  Inclusion path for the parents of the corresponding `InclusionPath[l][c]`.

Design notes:

- `CommRLast` is a public input, since we will be using it during Proof-of-Spacetime.
- `InclusionPath` and `ParentInclusionPath`: Each layer `l` has `LAYER_CHALLENGES[l]` inclusion paths.

## Private Inputs

*Inputs that the prover uses to generate a SNARK proof, these are not needed by the verifier to verify the proof*

- `CommR : [LAYERS-1]Fr`: Commitment of the the encoded data at each layer.

  Note: Size is `LAYERS-1` since the commitment to the last layer is `CommRLast`

- `DataProof : [LAYERS][][TREE_DEPTH]Fr`: Merkle tree inclusion proof for the current layer unencoded challenged leaf.

- `ReplicaProof : [LAYERS][][TREE_DEPTH]Fr`: Merkle tree inclusion proof for the current layer encoded challenged leaves.

- `ParentProof : [LAYERS][][PARENT_COUNT][TREE_DEPTH]Fr`: Pedersen hashes of the Merkle inclusion proofs of the parent leaves for each challenged leaf at layer `l`.

- `DataValue : [LAYERS][]Fr`: Value of the unencoded challenged leaves at layer `l`.

- `ReplicaValue : [LAYERS][]Fr`: Value of the encoded leaves for each challenged leaf at layer `l`.

- `ParentValue : [LAYERS][][PARENT_COUNT]Fr`: Value of the parent leaves for each challenged leaf at layer `l`.

## Circuit

In high level, we do 4 checks:

1. **ReplicaId Check**: Check the binary representation of the ReplicaId
2. **Inclusion Proofs Checks**: Check the inclusion proofs
3. **Encoding Checks**: Check that the data has been correctly encoding into a replica
4. **CommRStar Check**: Check that CommRStar has been generated correctly

Detailed

```go
// 1: ReplicaId Check - Check ReplicaId is equal to its bit representation
let ReplicaIdBits : [255]Fr = Fr_to_bits(ReplicaId)
assert(Packed(replica_id_bits) == ReplicaId)

let DataRoot, ReplicaRoot Fr

for l in range LAYERS {

  if l == 0 {
    DataRoot = CommD
  } else {
    DataRoot = CommR[l-1]
  }

  if l == LAYERS-1 {
    ReplicaRoot = CommRLast
  } else {
    ReplicaRoot = CommR[l]
  }

  for c in range LAYERS_CHALLENGES[l] {
    // 2: Inclusion Proofs Checks
    // 2.1: Check inclusion proofs for data leaves are correct
    assert(MerkleTreeVerify(DataRoot, InclusionPath[l][c], DataProof[l][c], DataValue[l][c]))
    // 2.2: Check inclusion proofs for replica leaves are correct
    assert(MerkleTreeVerify(ReplicaRoot, InclusionPath[l][c], ReplicaProof[l][c], ReplicaValue[l][c]))
    // 2.3: Check inclusion proofs for parent leaves are correct
    for p in range PARENT_COUNT {
      assert(MerkleTreeVerify(ReplicaRoot, ParentInclusionPath[l][c][p], ParentProof[l][c][p]))
    }

    // 3: Encoding checks - Check that replica leaves have been correctly encoded
    let ParentBits [PARENT_COUNT][255]Fr
    for p in range PARENT_COUNT {
      // 3.1: Check that each ParentValue is equal to its bit representation
      let parent = ParentValue[l][c][p]
      ParentBits[p] = Fr_to_bits(parent)
      assert(Packed(ParentBits[p]) == parent)
    }

    // 3.2: KDF check - Check that each key has generated correctly
    // PreImage = ReplicaIdBits || ParentBits[1] .. ParentBits[PARENT_NODES]
    let PreImage = ReplicaIdBits
    for parentbits in ParentBits {
      PreImage.Append(parentbits)
    }
    let key Fr = Blake2s(PreImage)
    assert(Blake2s(PreImage) == key)

    // 3.3: Check that the data has been encoded to a replica with the right key
    assert(ReplicaValue[l][c] == DataValue[l][c] + key)
  }

  // 4: CommRStar check - Check that the CommRStar constructed correctly
  let hash = ReplicaId
  for l in range LAYERS-1 {
    hash.Append(CommR[l])
  }
  hash.Append(CommRLast)

  assert(CommRStar == PedersenHash(hash))
  // TODO check if we need to do packing/unpacking
}
```



## Verification of offline porep proof

- SNARK proof check: **Check** that given the SNARK proof and the public inputs, the SNARK verification outputs true
- Parent checks: For each `leaf = InclusionPath[l][c]`:
  - **Check** that all `ParentsInclusionPaths_[l][c][0..PARENT_COUNT}` are the correct parent leaves of `leaf` in the DRG graph, if a leaf has less than `PARENT_COUNT`, repeat the leaf with the highest label in the graph.
  - **Check** that the parent leaves are in ascending numerical order.



## Layer Challenge Counts

TODO: define `Challenge` (or find existing definition)

TODO: we should just list current parameters and show this as a calculation for correctness, this should not mandatory to implement.

```go
func ChallengesForLayer(challenge Challenge, layer uint) uint {

    switch challenge.Type {
      // TODO: remove ambiguity, there should not be a "fixed" case
        case Fixed:
            return challenge.Count
        case Tapered:
      // TODO: current calculation is incorrect and does not match claim6 from Fisch2019, it should look more like: https://observablehq.com/d/bbabac1947b79011#gen_zigzag_taper
            assert(layer < LAYERS)
            l := (LAYERS - 1) - layer
            r := 1.0 - TAPER;
            t := min(l, TAPER_LAYERS)

            totalTaper := pow(r, t)

            calculated := ceil(totalTaper * challenge.count)

            // Although implied by the call to `ceil()` above, be explicit
            // that a layer cannot contain 0 challenges.
            max(1, calculated)
    }
}
```
